# LeyClara.IA üìú‚öñÔ∏è

**LeyClara.IA** es un asistente legal inteligente dise√±ado para democratizar el acceso a la informaci√≥n jur√≠dica. Su objetivo es tomar documentos legales complejos (leyes, contratos, sentencias) y explicarlos en un lenguaje sencillo y accesible para cualquier persona, "como si tuviera 5 a√±os".

## üß† Arquitectura T√©cnica: RAG (Retrieval-Augmented Generation)

Este proyecto utiliza una arquitectura **RAG** en lugar de un chat simple con IA.

### ¬øPor qu√© RAG?
Los modelos de lenguaje (LLMs) como GPT o Gemini tienen dos grandes limitaciones:
1.  **Alucinaciones:** Pueden inventar datos si no saben la respuesta.
2.  **Desconocimiento:** No conocen tus documentos privados o leyes locales espec√≠ficas que no estaban en su entrenamiento.

RAG soluciona esto permitiendo que la IA "lea" tus documentos antes de responder. El flujo es:
1.  **B√∫squeda (Retrieval):** El sistema busca en tu biblioteca los p√°rrafos exactos que responden a tu pregunta.
2.  **Generaci√≥n (Augmented Generation):** Env√≠a esos p√°rrafos a la IA y le dice: *"Usa SOLO esta informaci√≥n para responder al usuario"*. Esto garantiza respuestas precisas y basadas en evidencia real.

## üìö El Rol de ChromaDB (Nuestra "Biblioteca")

Para que el sistema sea r√°pido y eficiente, utilizamos **ChromaDB**, una base de datos vectorial.

*   **El Problema:** No podemos enviarle un PDF de 500 p√°ginas a la IA cada vez que hacemos una pregunta. Ser√≠a lento, costoso y exceder√≠a la memoria del modelo.
*   **La Soluci√≥n (ChromaDB):**
    1.  Cuando subes un PDF, lo "cortamos" en pedazos peque√±os.
    2.  Convertimos cada pedazo en una "huella digital matem√°tica" (vector) usando la API de Google Embeddings.
    3.  Guardamos estos vectores en ChromaDB localmente.
    4.  Cuando preguntas, ChromaDB encuentra matem√°ticamente los 6 fragmentos m√°s parecidos a tu pregunta en milisegundos, sin tener que volver a leer todo el documento.

## ‚úÇÔ∏è Estrategia de Chunking (Fragmentaci√≥n)

Una decisi√≥n cr√≠tica de dise√±o fue el tama√±o de los "chunks" (fragmentos de texto).

*   **Configuraci√≥n Actual:** 500 caracteres (con 100 de solapamiento).
*   **¬øPor qu√© este tama√±o?**
    *   Inicialmente probamos con 1000 caracteres, pero el sistema perd√≠a detalles espec√≠ficos (como n√∫meros de art√≠culos o t√≠tulos cortos) porque se "dilu√≠an" en tanto texto.
    *   Al reducirlo a **500 caracteres**, logramos un efecto "lupa": cada fragmento es m√°s espec√≠fico y preciso. Esto permite encontrar "agujas en un pajar" (detalles puntuales) con mucha mayor eficacia.

## ÔøΩ El Rol de LangChain (El "Pegamento")

**LangChain** es el framework que conecta todas las piezas del rompecabezas. Act√∫a como el orquestador que:

1.  **Carga y Procesa:** Usa herramientas para leer PDFs y dividirlos en chunks.
2.  **Conecta:** Sirve de puente entre tu base de datos local (ChromaDB) y la API de Google (Gemini).
3.  **Gestiona el Flujo:** Cuando haces una pregunta, LangChain ejecuta autom√°ticamente una "cadena" de pasos: buscar contexto -> construir el prompt -> consultar a la IA -> entregar la respuesta.

Sin LangChain, tendr√≠amos que escribir manualmente todo el c√≥digo para conectar estos servicios dispares.

## ÔøΩüõ†Ô∏è Tecnolog√≠as

*   **Backend:** Python, FastAPI.
*   **IA:** Google Gemini 1.5 Flash (v√≠a LangChain).
*   **Base de Datos:** ChromaDB (Vector Store).
*   **Frontend:** React, TailwindCSS.
*   **Infraestructura:** Docker & Docker Compose.

## üöÄ C√≥mo Ejecutarlo

1.  Clona el repositorio.
2.  Crea un archivo `.env` con tus claves de API (ver `.env.example`).
3.  Ejecuta:
    ```bash
    docker-compose up --build
    ```
4.  Abre `http://localhost:3000` y empieza a subir documentos.
